<!DOCTYPE html>
<html lang="en">
  <!-- Head tag -->
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Title -->
  
  <title>Speech Synthesis Toolkits Introduction - Xiao Zhang (Software Design Office)</title>

  <!--Favicon-->
  <link rel="icon" href="favicon/favicon.ico">

  <!--Description-->
  
      <meta name="description" content="About TTS (Text to Speech)The first thing is about text analysis process

Word division, annotation, lexical annotation, rhyme prediction, etc.
Repres">
  

  <!--Author-->
  
      <meta name="author" content="Xiao Zhang">
  

  <!-- Pure CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
  <link href="https://fonts.googleapis.com/css?family=Crimson+Text|Open+Sans:300,800" rel="stylesheet">

  <!-- Custom CSS -->
  
<link rel="stylesheet" href="/css/styles.css">


  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <!-- Google Analytics -->
  

<meta name="generator" content="Hexo 6.2.0"></head>


  <body>
  	<div class="container-fluid navbar-container m-sm-5">
      <!-- Header -->
      <nav class="navbar navbar-toggleable-sm navbar-light px-1 py-3 my-3 mb-sm-5">
  <a class="navbar-brand ml-2" href="/">Xiao Zhang (Software Design Office)</a>
  <button class="navbar-toggler navbar-toggler-right py-2" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse text-center" id="navbarCollapse">
    <ul class="navbar-nav ml-auto my-auto">
      
        <li class="nav-item">
          <a class="nav-link" href="/about">About</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" target="_blank" rel="noopener" href="https://www.linkedin.com/in/xiao-zhang-28785b19b/">LinkedIn</a>
        </li>
      
    </ul>
    <hr class="hidden-md-up" />
  </div>
</nav>


  		<div class="row">
  			<!-- <div class="col-12 mb-4"> -->
<!--   <img class="img-fluid project-img" src="/" alt="Speech Synthesis Toolkits Introduction"> -->
<!-- </div> -->
<!-- <div class="col-lg-4 col-12 pt-3 px-4 pr-lg-5"> -->
<!--   <h1>Speech Synthesis Toolkits Introduction</h1> -->
<!-- </div> -->
<div class="col-lg-8 col-12 pt-lg-3 mb-4 pl-lg-5 px-lg-0 px-4 portfolio-content">
  <h1 id="About-TTS-Text-to-Speech"><a href="#About-TTS-Text-to-Speech" class="headerlink" title="About TTS (Text to Speech)"></a>About TTS (Text to Speech)</h1><p>The first thing is about text analysis process</p>
<ul>
<li>Word division, annotation, lexical annotation, rhyme prediction, etc.</li>
<li>Representation of normalized transcripts as phoneme-level text features</li>
<li>The length M of text features is only related to the transcript itself, not to the speech duration.</li>
</ul>
<p>We are usually talk about the stastical Speech Synthesis. After we doing the text analysis, we will just get into the acoustic generation part. </p>
<h4 id="Duration-Model"><a href="#Duration-Model" class="headerlink" title="Duration Model"></a>Duration Model</h4><p>The input will be the HMM output labels and transform into one-hot features, along with the statistical features.</p>
<p>Output will be the phone duration and the time of the states. In HMM,  we will get a frame level alignment. </p>
<h4 id="Acoustic-Model"><a href="#Acoustic-Model" class="headerlink" title="Acoustic Model"></a>Acoustic Model</h4><p>The input will be the frame level info, the output will be the spectrum params, the f0, along with the vowels, and consonants. After feeding those params into the vocoder, we will get the speech. </p>
<p>For every statistical model, the most important parts can be: the <code>acoustic model</code>, and <code>vocoder</code>.</p>
<h2 id="Acoustic-Generation"><a href="#Acoustic-Generation" class="headerlink" title="Acoustic Generation"></a>Acoustic Generation</h2><h3 id="HMM-based-acoustic-model"><a href="#HMM-based-acoustic-model" class="headerlink" title="HMM based acoustic model"></a>HMM based acoustic model</h3><h4 id="Text-Analysis"><a href="#Text-Analysis" class="headerlink" title="Text Analysis"></a>Text Analysis</h4><p>Here the text shows the start time and end time of the phone starts and stops. The start time and end time is from the HMM model alignment. And in decoding part we can decode the time for each phone.  </p>
<p><img src="/../images/image-20220826234317040.png" alt="This "></p>
<h3 id="NN-seq2seq-end2end-based-acoustic-model-Tactron"><a href="#NN-seq2seq-end2end-based-acoustic-model-Tactron" class="headerlink" title="NN(seq2seq, end2end) based acoustic model - Tactron"></a>NN(seq2seq, end2end) based acoustic model - Tactron</h3><h3 id="Text-Analysis-1"><a href="#Text-Analysis-1" class="headerlink" title="Text Analysis"></a>Text Analysis</h3><p>And this is the full information to show how each phone’s prodacy infomation. That is how it different from the HMM model since it does not need to really record all the starting point and ending point of each phone. Thats the core difference between the previous model and latest way. </p>
<p><img src="/../images/image-20220826234258547.png" alt="The Prodoty Text Trainning Corpus"></p>
<p>In tactron, we can just use the CBHG to do the pre-decoding, to learn the all textual information for the trainning text corpus, like grammar or semantic information. </p>
<p>There are the basic components in Tactron.</p>
<p><img src="/../images/image-20220827001951827.png" alt="2017. Wang et.al. - Tacotron: Towards end-to-end speech synthesis."></p>
<p>The input text will be the English sentences, for Chinese will just be the Chinese phones. It will use the Mel filter to get the Mel charactersitics. </p>
<h1 id="Tacotron-Training"><a href="#Tacotron-Training" class="headerlink" title="Tacotron Training"></a>Tacotron Training</h1><h2 id="Implementing-CBHG-Encoder-in-Tacotron"><a href="#Implementing-CBHG-Encoder-in-Tacotron" class="headerlink" title="Implementing CBHG Encoder in Tacotron"></a>Implementing CBHG Encoder in Tacotron</h2><p>The Tacotron model based on attention mechanism includes two parts, encoder and decoder with attention mechanism, and this assignment only needs to implement the CBHG-based encoder part.</p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>Based on 10 hours of Chinese data open-sourced by Beibei Technology, we provide the processed text features and packaged them together with the audio files to Baidu.com, download link.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Link: https://pan.baidu.com/s/1xC0fNwDvJWpJdqnfqNAzMg Password: tgtp</span><br></pre></td></tr></table></figure>

<p>In the tacotron directory in testdata, we also provide some data from this database directly to test the process.</p>
<h3 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h3><p>First execute <code>pip install -r requirement.txt</code> to install the required environment, which is also the test environment for this job.</p>
<p>Due to the large number of parameters in this network, it is recommended to use a server with a GPU for training tests.</p>
<h3 id="Procedure"><a href="#Procedure" class="headerlink" title="Procedure"></a>Procedure</h3><p>First, in the egs&#x2F;example directory, run <code>bash preprocess.sh 4</code> to extract features from the text and speech data in the target testdata.</p>
<p>After the feature extraction, run <code>bash train.sh</code> to train the model. The error is in models&#x2F;basic_model.py, which is where we need to implement the CBHG encoder part of Tacotron.</p>
<p>Once the CBHG implementation is complete, training can be performed directly.</p>
<p>After the training is complete, a synthesis script is written to test it, modeled after egs&#x2F;example&#x2F;synthesis.sh. Since the attention mechanism used at this point is the most primitive attention mechanism mentioned in the code explanation, the convergence performance and effect are poor. In the next assignment we will implement a more stable Tacotron system.</p>
<h1 id="Traditional-TTS-model-Training-Demo-duration-model-acoustic-model"><a href="#Traditional-TTS-model-Training-Demo-duration-model-acoustic-model" class="headerlink" title="Traditional TTS model Training Demo(duration model, acoustic model)"></a>Traditional TTS model Training Demo(duration model, acoustic model)</h1><p>Text features -&gt; duration model -&gt; acoustic model -&gt; world vocoder -&gt; audio</p>
<h2 id="1、Download-data-configure-environment"><a href="#1、Download-data-configure-environment" class="headerlink" title="1、Download data, configure environment"></a>1、Download data, configure environment</h2><h3 id="Download-data"><a href="#Download-data" class="headerlink" title="Download data"></a>Download data</h3><p>Data link: <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1_zN-PSIIrxtCGvjo1TWz1g">https://pan.baidu.com/s/1_zN-PSIIrxtCGvjo1TWz1g</a></p>
<p>Extraction code: jbc5</p>
<p>Unzip the train_data folder</p>
<p>Data link: <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1i28ZppgWYHIupk8piH7SyQ">https://pan.baidu.com/s/1i28ZppgWYHIupk8piH7SyQ</a></p>
<p>Extraction code: mvkj</p>
<h3 id="Configure-environment-Python-3-6-Tensorflow-1-12"><a href="#Configure-environment-Python-3-6-Tensorflow-1-12" class="headerlink" title="Configure environment (Python 3.6 Tensorflow 1.12)"></a>Configure environment (Python 3.6 Tensorflow 1.12)</h3><p>Install python package</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<p>If you are slow to install, you can use whl to install</p>
<p>tensorflow 1.12 whl package: <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1WCOyFhszJnHHtIMWBq0sxQ">https://pan.baidu.com/s/1WCOyFhszJnHHtIMWBq0sxQ</a></p>
<p>Extraction code: r73i</p>
<h2 id="2-Normalize-data"><a href="#2-Normalize-data" class="headerlink" title="2. Normalize data"></a>2. Normalize data</h2><p>Normalize the duration and acoustic input and output data to get the train_cmvn_dur.npz and train_cmvn_spss.npz files in cmvn.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash bash/prepare.sh</span><br></pre></td></tr></table></figure>

<h2 id="3-Write-the-model"><a href="#3-Write-the-model" class="headerlink" title="3. Write the model"></a>3. Write the model</h2><p>Complete the <code>AcousticModel</code> and <code>DurationModel</code> model definition section in model.py</p>
<p>The model inputs are inputs and input_length, and the predicted results are targets</p>
<p>inputs are the input features, input_length is the first dimension of inputs, targets is the prediction result</p>
<h3 id="Inputs-and-outputs"><a href="#Inputs-and-outputs" class="headerlink" title="Inputs and outputs"></a>Inputs and outputs</h3><p><code>inputs.shape</code> &#x3D; <code>[seq_length, feature_dim]</code></p>
<p><code>input_length</code> &#x3D; <code>seq_length</code></p>
<p><code>target.shape</code> &#x3D; <code>[seq_length, target_dim]</code></p>
<p>The input feature_dim of the time-length model is <code>617</code> dimensions, which represents the text feature.</p>
<p>The target_dim of the output of the duration model is <code>5</code> dimensions, which represents the state duration information of each phoneme</p>
<p>The input of the acoustic model has a feature_dim of <code>626</code> dimensions, which represents the text features and the position features of the frame</p>
<p>The output of the acoustic model has a target_dim of <code>75</code> dimensions, which represents the acoustic features of the target audio (lf0,mgc,bap)</p>
<h3 id="Task-description"><a href="#Task-description" class="headerlink" title="Task description"></a>Task description</h3><p>Write the model to predict the output targets based on the input inputs</p>
<h2 id="4-Training"><a href="#4-Training" class="headerlink" title="4. Training"></a>4. Training</h2><p>Train the duration model</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash bash/train_dur.sh</span><br></pre></td></tr></table></figure>

<p>Train the acoustic model</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash bash/train_acoustic.sh</span><br></pre></td></tr></table></figure>

<p>The training model results are saved in logdir_dur and logdir_acoustic respectively</p>
<p>The total number of training steps, checkpoint steps, etc. can be modified in hparams.py. The model convergence can be judged by the loss curve and the effect of test synthesis, not necessarily by the total number of steps run.</p>
<h3 id="the-loss-function-curve-by-tensorborad"><a href="#the-loss-function-curve-by-tensorborad" class="headerlink" title="the loss function curve by tensorborad"></a>the loss function curve by tensorborad</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=logdir_dur</span><br></pre></td></tr></table></figure>

<p>Open browser to view local port <code>6006</code></p>
<h2 id="5-Test-synthesis"><a href="#5-Test-synthesis" class="headerlink" title="5. Test synthesis"></a>5. Test synthesis</h2><p>The predicted output of the duration model is in output_dur (as input to the acoustic model)</p>
<p>The predicted result of the acoustic model is in output_acoustic (lf0, bap, mgc folders for the corresponding features generated, according to which the audio in syn_wav is synthesized by the world vocoder)</p>
<p>&lt;checkpoint&gt; Fill in the path of the model obtained by training</p>
<h3 id="1-Test-the-acoustic-model-using-real-duration-data"><a href="#1-Test-the-acoustic-model-using-real-duration-data" class="headerlink" title="1. Test the acoustic model (using real duration data)"></a>1. Test the acoustic model (using real duration data)</h3><p>The first parameter of the test script is the input label path, the second parameter is the output path, and the third parameter is the trained model path (e.g.: logdir_acoustic&#x2F;model&#x2F;model.ckpt-2000)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash bash/synthesize_acoustic.sh test_data/acoustic_features output_acoustic &lt;checkpoint&gt;</span><br></pre></td></tr></table></figure>

<h3 id="2-Test-duration-model-acoustic-model-use-the-output-of-the-duration-model-as-input-to-the-acoustic-model"><a href="#2-Test-duration-model-acoustic-model-use-the-output-of-the-duration-model-as-input-to-the-acoustic-model" class="headerlink" title="2. Test duration model + acoustic model (use the output of the duration model as input to the acoustic model)"></a>2. Test duration model + acoustic model (use the output of the duration model as input to the acoustic model)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bash bash/synthesize_dur.sh test_data/duration_features output_dur &lt;checkpoint</span><br><span class="line">bash bash/synthesize_acoustic.sh output_dur output_acoustic &lt;checkpoint&gt;</span><br></pre></td></tr></table></figure>

<p>The final synthesized voice is in output_acoustic&#x2F;syn_wav</p>
<h2 id="Vocoder"><a href="#Vocoder" class="headerlink" title="Vocoder"></a>Vocoder</h2><h3 id="Source-Filter-Vocoder-Use-World-as-an-example"><a href="#Source-Filter-Vocoder-Use-World-as-an-example" class="headerlink" title="Source Filter Vocoder: Use World as an example"></a>Source Filter Vocoder: Use World as an example</h3><p>Source Filter Vocoder made use of the source filter theory. There has some open-resource vocoders like:</p>
<p>HTS Vocoder</p>
<p>Griffin-Lim Vocoder: Converst Mel Spectogram into the speech audio file.</p>
<p>Staright: <a target="_blank" rel="noopener" href="https://github.com/shuaijiang/STRAIGHT">https://github.com/shuaijiang/STRAIGHT</a> <a target="_blank" rel="noopener" href="http://www.isc.meiji.ac.jp/~mmorise/straight/english/introductions.html">http://www.isc.meiji.ac.jp/~mmorise/straight/english/introductions.html</a></p>
<p>World: <a target="_blank" rel="noopener" href="https://github.com/mmorise/World">https://github.com/mmorise/World</a></p>
<p>World is a very popular and latest open resource vocoder project on github. WORLD corresponds to the following three acoustic characteristics: F0 fundamental frequency (F0基频), SP spectral envelope（SP频谱包络） and AP non-periodic sequence （AP非周期序列）.</p>
<p>The lowest frequency sine wave of the original signal composed of sine waves is the fundamental frequency.</p>
<p>Spectral envelope is the envelope obtained by connecting the highest points of amplitudes of different frequencies by a smooth curve.</p>
<p>The non-periodic sequence corresponds to the non-periodic pulse sequence of the mixed excitation part.</p>
<h3 id="Install-the-World"><a href="#Install-the-World" class="headerlink" title="Install the World"></a>Install the World</h3><p><strong>git clone</strong> </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/xiao11lam/TTS.git</span><br><span class="line"><span class="built_in">cd</span> TTS/03_spss/tools</span><br></pre></td></tr></table></figure>

<p><strong>compile</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> tools</span><br><span class="line">bash compile_tools.sh</span><br></pre></td></tr></table></figure>

<p>The compilation process may take a while, so please be patient and get the tools&#x2F;bin folder after compiling</p>
<p><img src="/../images/image-20220826021655278.png" alt="image-20220826021655278"></p>
<h3 id="copy-synthesis"><a href="#copy-synthesis" class="headerlink" title="copy synthesis"></a>copy synthesis</h3><p>Synthesize syn.wav with world vocoder based on input test.wav</p>
<p>Extract <code>f0</code>, <code>sp</code>, <code>ap</code> from <code>test.wav</code> using world, then synthesize <code>copy_synthesize/16k_wav_syn/000001.resyn.wav</code> based on the extracted features</p>
<h3 id="Sample-rate-16k"><a href="#Sample-rate-16k" class="headerlink" title="Sample rate 16k"></a>Sample rate 16k</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash copy_synthesize/copy_synthesis_world_16k.sh</span><br></pre></td></tr></table></figure>

<h3 id="sample-rate-48k"><a href="#sample-rate-48k" class="headerlink" title="sample rate 48k"></a>sample rate 48k</h3><p>Modify on the basis of <code>copy_synthesis_world_16k.sh</code>, you can modify parameters as input and output paths <code>(wav_dir\out_dir)</code>, <code>sample rate fs</code>, <code>mcsize</code>, etc</p>
<h3 id="optional-melspectrogram-copy-synthesis"><a href="#optional-melspectrogram-copy-synthesis" class="headerlink" title="(optional) melspectrogram copy synthesis"></a>(optional) melspectrogram copy synthesis</h3><p>Synthesize <code>syn_mel.wav</code> from input <code>test.wav</code> via <code>griffinlim</code></p>
<p>Extract the melspectrogram of test.wav using world, then synthesize copy_synthesize&#x2F;syn_mel.wav based on the extracted melspectrogram features</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python copy_synthesize/copy_synthesis_mel.py copy_synthesize/16k_wav/000001.wav</span><br></pre></td></tr></table></figure>





<h3 id="NN-based-Vocoder"><a href="#NN-based-Vocoder" class="headerlink" title="NN based Vocoder"></a>NN based Vocoder</h3><h1 id="Tacotron-TTS"><a href="#Tacotron-TTS" class="headerlink" title="Tacotron TTS"></a>Tacotron TTS</h1><p><img src="/../images/image-20220825235943493.png" alt="image-20220825235943493"></p>
<h2 id="Tacotron-Hands-on"><a href="#Tacotron-Hands-on" class="headerlink" title="Tacotron Hands-on"></a>Tacotron Hands-on</h2><h3 id="Clone-the-code"><a href="#Clone-the-code" class="headerlink" title="Clone the code"></a>Clone the code</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/xiao11lam/TTS.git</span><br></pre></td></tr></table></figure>

<p>Or we can clone here:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/nwpuaslp/TTS_Course.git</span><br></pre></td></tr></table></figure>











<h1 id="ESPNET-Text-to-Speech"><a href="#ESPNET-Text-to-Speech" class="headerlink" title="ESPNET Text to Speech"></a>ESPNET Text to Speech</h1><h2 id="Install-ESPNET"><a href="#Install-ESPNET" class="headerlink" title="Install ESPNET"></a>Install ESPNET</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name espnet</span><br></pre></td></tr></table></figure>


</div>


<style>
  .portfolio-content img {
    width: 100%;      /* Make the image width equal to the container width */
    height: auto;     /* Maintain the aspect ratio */
    max-width: 100%;  /* Ensures the image does not exceed the container width */
  }
</style>





      </div>
      
  	</div>

    <!-- After footer scripts -->
    <script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>

  </body>
</html>

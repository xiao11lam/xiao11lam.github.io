

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Xiao Zhang">
  <meta name="keywords" content="Software Architecture Design">
  
    <meta name="description" content="About TTS (Text to Speech)The first thing is about text analysis process  Word division, annotation, lexical annotation, rhyme prediction, etc. Representation of normalized transcripts as phoneme-leve">
<meta property="og:type" content="article">
<meta property="og:title" content="Speech Synthesis Toolkits Introduction">
<meta property="og:url" content="http://xiaos.site/2022/08/25/Speech-Synthesis-Toolkits-Introduction/index.html">
<meta property="og:site_name" content="Xiao">
<meta property="og:description" content="About TTS (Text to Speech)The first thing is about text analysis process  Word division, annotation, lexical annotation, rhyme prediction, etc. Representation of normalized transcripts as phoneme-leve">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://xiaos.site/images/image-20220826234317040.png">
<meta property="og:image" content="http://xiaos.site/images/image-20220826234258547.png">
<meta property="og:image" content="http://xiaos.site/images/image-20220827001951827.png">
<meta property="og:image" content="http://xiaos.site/images/image-20220826021655278.png">
<meta property="og:image" content="http://xiaos.site/images/image-20220825235943493.png">
<meta property="article:published_time" content="2022-08-25T22:33:24.000Z">
<meta property="article:modified_time" content="2024-11-25T22:45:14.893Z">
<meta property="article:author" content="Xiao Zhang">
<meta property="article:tag" content="Software Architecture Design">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://xiaos.site/images/image-20220826234317040.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Speech Synthesis Toolkits Introduction - Xiao</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/css/fluid-extention.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"xiaos.site","root":"/","version":"1.9.1","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Xiao @ Speech and Language Processing</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archive/">
                <i class="iconfont icon-archive-fill"></i>
                Archive
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/products/">
                <i class="iconfont icon-category-fill"></i>
                Products
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Use Cases
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                Links
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/banner.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Speech Synthesis Toolkits Introduction"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-08-25 23:33" pubdate>
          August 25, 2022 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          8.1k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          68 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Speech Synthesis Toolkits Introduction</h1>
            
            <div class="markdown-body">
              
              <h1 id="About-TTS-Text-to-Speech"><a href="#About-TTS-Text-to-Speech" class="headerlink" title="About TTS (Text to Speech)"></a>About TTS (Text to Speech)</h1><p>The first thing is about text analysis process</p>
<ul>
<li>Word division, annotation, lexical annotation, rhyme prediction, etc.</li>
<li>Representation of normalized transcripts as phoneme-level text features</li>
<li>The length M of text features is only related to the transcript itself, not to the speech duration.</li>
</ul>
<p>We are usually talk about the stastical Speech Synthesis. After we doing the text analysis, we will just get into the acoustic generation part. </p>
<h4 id="Duration-Model"><a href="#Duration-Model" class="headerlink" title="Duration Model"></a>Duration Model</h4><p>The input will be the HMM output labels and transform into one-hot features, along with the statistical features.</p>
<p>Output will be the phone duration and the time of the states. In HMM,  we will get a frame level alignment. </p>
<h4 id="Acoustic-Model"><a href="#Acoustic-Model" class="headerlink" title="Acoustic Model"></a>Acoustic Model</h4><p>The input will be the frame level info, the output will be the spectrum params, the f0, along with the vowels, and consonants. After feeding those params into the vocoder, we will get the speech. </p>
<p>For every statistical model, the most important parts can be: the <code>acoustic model</code>, and <code>vocoder</code>.</p>
<h2 id="Acoustic-Generation"><a href="#Acoustic-Generation" class="headerlink" title="Acoustic Generation"></a>Acoustic Generation</h2><h3 id="HMM-based-acoustic-model"><a href="#HMM-based-acoustic-model" class="headerlink" title="HMM based acoustic model"></a>HMM based acoustic model</h3><h4 id="Text-Analysis"><a href="#Text-Analysis" class="headerlink" title="Text Analysis"></a>Text Analysis</h4><p>Here the text shows the start time and end time of the phone starts and stops. The start time and end time is from the HMM model alignment. And in decoding part we can decode the time for each phone.  </p>
<p><img src="/../images/image-20220826234317040.png" srcset="/img/loading.gif" lazyload alt="This "></p>
<h3 id="NN-seq2seq-end2end-based-acoustic-model-Tactron"><a href="#NN-seq2seq-end2end-based-acoustic-model-Tactron" class="headerlink" title="NN(seq2seq, end2end) based acoustic model - Tactron"></a>NN(seq2seq, end2end) based acoustic model - Tactron</h3><h3 id="Text-Analysis-1"><a href="#Text-Analysis-1" class="headerlink" title="Text Analysis"></a>Text Analysis</h3><p>And this is the full information to show how each phone’s prodacy infomation. That is how it different from the HMM model since it does not need to really record all the starting point and ending point of each phone. Thats the core difference between the previous model and latest way. </p>
<p><img src="/../images/image-20220826234258547.png" srcset="/img/loading.gif" lazyload alt="The Prodoty Text Trainning Corpus"></p>
<p>In tactron, we can just use the CBHG to do the pre-decoding, to learn the all textual information for the trainning text corpus, like grammar or semantic information. </p>
<p>There are the basic components in Tactron.</p>
<p><img src="/../images/image-20220827001951827.png" srcset="/img/loading.gif" lazyload alt="2017. Wang et.al. - Tacotron: Towards end-to-end speech synthesis."></p>
<p>The input text will be the English sentences, for Chinese will just be the Chinese phones. It will use the Mel filter to get the Mel charactersitics. </p>
<h1 id="Tacotron-Training"><a href="#Tacotron-Training" class="headerlink" title="Tacotron Training"></a>Tacotron Training</h1><h2 id="Implementing-CBHG-Encoder-in-Tacotron"><a href="#Implementing-CBHG-Encoder-in-Tacotron" class="headerlink" title="Implementing CBHG Encoder in Tacotron"></a>Implementing CBHG Encoder in Tacotron</h2><p>The Tacotron model based on attention mechanism includes two parts, encoder and decoder with attention mechanism, and this assignment only needs to implement the CBHG-based encoder part.</p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>Based on 10 hours of Chinese data open-sourced by Beibei Technology, we provide the processed text features and packaged them together with the audio files to Baidu.com, download link.</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">Link: https:<span class="hljs-regexp">//</span>pan.baidu.com<span class="hljs-regexp">/s/</span><span class="hljs-number">1</span>xC0fNwDvJWpJdqnfqNAzMg Password: tgtp<br></code></pre></td></tr></table></figure>

<p>In the tacotron directory in testdata, we also provide some data from this database directly to test the process.</p>
<h3 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h3><p>First execute <code>pip install -r requirement.txt</code> to install the required environment, which is also the test environment for this job.</p>
<p>Due to the large number of parameters in this network, it is recommended to use a server with a GPU for training tests.</p>
<h3 id="Procedure"><a href="#Procedure" class="headerlink" title="Procedure"></a>Procedure</h3><p>First, in the egs&#x2F;example directory, run <code>bash preprocess.sh 4</code> to extract features from the text and speech data in the target testdata.</p>
<p>After the feature extraction, run <code>bash train.sh</code> to train the model. The error is in models&#x2F;basic_model.py, which is where we need to implement the CBHG encoder part of Tacotron.</p>
<p>Once the CBHG implementation is complete, training can be performed directly.</p>
<p>After the training is complete, a synthesis script is written to test it, modeled after egs&#x2F;example&#x2F;synthesis.sh. Since the attention mechanism used at this point is the most primitive attention mechanism mentioned in the code explanation, the convergence performance and effect are poor. In the next assignment we will implement a more stable Tacotron system.</p>
<h1 id="Traditional-TTS-model-Training-Demo-duration-model-acoustic-model"><a href="#Traditional-TTS-model-Training-Demo-duration-model-acoustic-model" class="headerlink" title="Traditional TTS model Training Demo(duration model, acoustic model)"></a>Traditional TTS model Training Demo(duration model, acoustic model)</h1><p>Text features -&gt; duration model -&gt; acoustic model -&gt; world vocoder -&gt; audio</p>
<h2 id="1、Download-data-configure-environment"><a href="#1、Download-data-configure-environment" class="headerlink" title="1、Download data, configure environment"></a>1、Download data, configure environment</h2><h3 id="Download-data"><a href="#Download-data" class="headerlink" title="Download data"></a>Download data</h3><p>Data link: <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1_zN-PSIIrxtCGvjo1TWz1g">https://pan.baidu.com/s/1_zN-PSIIrxtCGvjo1TWz1g</a></p>
<p>Extraction code: jbc5</p>
<p>Unzip the train_data folder</p>
<p>Data link: <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1i28ZppgWYHIupk8piH7SyQ">https://pan.baidu.com/s/1i28ZppgWYHIupk8piH7SyQ</a></p>
<p>Extraction code: mvkj</p>
<h3 id="Configure-environment-Python-3-6-Tensorflow-1-12"><a href="#Configure-environment-Python-3-6-Tensorflow-1-12" class="headerlink" title="Configure environment (Python 3.6 Tensorflow 1.12)"></a>Configure environment (Python 3.6 Tensorflow 1.12)</h3><p>Install python package</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install -r requirements.txt<br></code></pre></td></tr></table></figure>

<p>If you are slow to install, you can use whl to install</p>
<p>tensorflow 1.12 whl package: <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1WCOyFhszJnHHtIMWBq0sxQ">https://pan.baidu.com/s/1WCOyFhszJnHHtIMWBq0sxQ</a></p>
<p>Extraction code: r73i</p>
<h2 id="2-Normalize-data"><a href="#2-Normalize-data" class="headerlink" title="2. Normalize data"></a>2. Normalize data</h2><p>Normalize the duration and acoustic input and output data to get the train_cmvn_dur.npz and train_cmvn_spss.npz files in cmvn.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash bash/prepare.sh<br></code></pre></td></tr></table></figure>

<h2 id="3-Write-the-model"><a href="#3-Write-the-model" class="headerlink" title="3. Write the model"></a>3. Write the model</h2><p>Complete the <code>AcousticModel</code> and <code>DurationModel</code> model definition section in model.py</p>
<p>The model inputs are inputs and input_length, and the predicted results are targets</p>
<p>inputs are the input features, input_length is the first dimension of inputs, targets is the prediction result</p>
<h3 id="Inputs-and-outputs"><a href="#Inputs-and-outputs" class="headerlink" title="Inputs and outputs"></a>Inputs and outputs</h3><p><code>inputs.shape</code> &#x3D; <code>[seq_length, feature_dim]</code></p>
<p><code>input_length</code> &#x3D; <code>seq_length</code></p>
<p><code>target.shape</code> &#x3D; <code>[seq_length, target_dim]</code></p>
<p>The input feature_dim of the time-length model is <code>617</code> dimensions, which represents the text feature.</p>
<p>The target_dim of the output of the duration model is <code>5</code> dimensions, which represents the state duration information of each phoneme</p>
<p>The input of the acoustic model has a feature_dim of <code>626</code> dimensions, which represents the text features and the position features of the frame</p>
<p>The output of the acoustic model has a target_dim of <code>75</code> dimensions, which represents the acoustic features of the target audio (lf0,mgc,bap)</p>
<h3 id="Task-description"><a href="#Task-description" class="headerlink" title="Task description"></a>Task description</h3><p>Write the model to predict the output targets based on the input inputs</p>
<h2 id="4-Training"><a href="#4-Training" class="headerlink" title="4. Training"></a>4. Training</h2><p>Train the duration model</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash bash/train_dur.sh<br></code></pre></td></tr></table></figure>

<p>Train the acoustic model</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash bash/train_acoustic.sh<br></code></pre></td></tr></table></figure>

<p>The training model results are saved in logdir_dur and logdir_acoustic respectively</p>
<p>The total number of training steps, checkpoint steps, etc. can be modified in hparams.py. The model convergence can be judged by the loss curve and the effect of test synthesis, not necessarily by the total number of steps run.</p>
<h3 id="the-loss-function-curve-by-tensorborad"><a href="#the-loss-function-curve-by-tensorborad" class="headerlink" title="the loss function curve by tensorborad"></a>the loss function curve by tensorborad</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tensorboard --logdir=logdir_dur<br></code></pre></td></tr></table></figure>

<p>Open browser to view local port <code>6006</code></p>
<h2 id="5-Test-synthesis"><a href="#5-Test-synthesis" class="headerlink" title="5. Test synthesis"></a>5. Test synthesis</h2><p>The predicted output of the duration model is in output_dur (as input to the acoustic model)</p>
<p>The predicted result of the acoustic model is in output_acoustic (lf0, bap, mgc folders for the corresponding features generated, according to which the audio in syn_wav is synthesized by the world vocoder)</p>
<p>&lt;checkpoint&gt; Fill in the path of the model obtained by training</p>
<h3 id="1-Test-the-acoustic-model-using-real-duration-data"><a href="#1-Test-the-acoustic-model-using-real-duration-data" class="headerlink" title="1. Test the acoustic model (using real duration data)"></a>1. Test the acoustic model (using real duration data)</h3><p>The first parameter of the test script is the input label path, the second parameter is the output path, and the third parameter is the trained model path (e.g.: logdir_acoustic&#x2F;model&#x2F;model.ckpt-2000)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash bash/synthesize_acoustic.sh test_data/acoustic_features output_acoustic &lt;checkpoint&gt;<br></code></pre></td></tr></table></figure>

<h3 id="2-Test-duration-model-acoustic-model-use-the-output-of-the-duration-model-as-input-to-the-acoustic-model"><a href="#2-Test-duration-model-acoustic-model-use-the-output-of-the-duration-model-as-input-to-the-acoustic-model" class="headerlink" title="2. Test duration model + acoustic model (use the output of the duration model as input to the acoustic model)"></a>2. Test duration model + acoustic model (use the output of the duration model as input to the acoustic model)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash bash/synthesize_dur.sh test_data/duration_features output_dur &lt;checkpoint<br>bash bash/synthesize_acoustic.sh output_dur output_acoustic &lt;checkpoint&gt;<br></code></pre></td></tr></table></figure>

<p>The final synthesized voice is in output_acoustic&#x2F;syn_wav</p>
<h2 id="Vocoder"><a href="#Vocoder" class="headerlink" title="Vocoder"></a>Vocoder</h2><h3 id="Source-Filter-Vocoder-Use-World-as-an-example"><a href="#Source-Filter-Vocoder-Use-World-as-an-example" class="headerlink" title="Source Filter Vocoder: Use World as an example"></a>Source Filter Vocoder: Use World as an example</h3><p>Source Filter Vocoder made use of the source filter theory. There has some open-resource vocoders like:</p>
<p>HTS Vocoder</p>
<p>Griffin-Lim Vocoder: Converst Mel Spectogram into the speech audio file.</p>
<p>Staright: <a target="_blank" rel="noopener" href="https://github.com/shuaijiang/STRAIGHT">https://github.com/shuaijiang/STRAIGHT</a> <a target="_blank" rel="noopener" href="http://www.isc.meiji.ac.jp/~mmorise/straight/english/introductions.html">http://www.isc.meiji.ac.jp/~mmorise/straight/english/introductions.html</a></p>
<p>World: <a target="_blank" rel="noopener" href="https://github.com/mmorise/World">https://github.com/mmorise/World</a></p>
<p>World is a very popular and latest open resource vocoder project on github. WORLD corresponds to the following three acoustic characteristics: F0 fundamental frequency (F0基频), SP spectral envelope（SP频谱包络） and AP non-periodic sequence （AP非周期序列）.</p>
<p>The lowest frequency sine wave of the original signal composed of sine waves is the fundamental frequency.</p>
<p>Spectral envelope is the envelope obtained by connecting the highest points of amplitudes of different frequencies by a smooth curve.</p>
<p>The non-periodic sequence corresponds to the non-periodic pulse sequence of the mixed excitation part.</p>
<h3 id="Install-the-World"><a href="#Install-the-World" class="headerlink" title="Install the World"></a>Install the World</h3><p><strong>git clone</strong> </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/xiao11lam/TTS.git<br><span class="hljs-built_in">cd</span> TTS/03_spss/tools<br></code></pre></td></tr></table></figure>

<p><strong>compile</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> tools<br>bash compile_tools.sh<br></code></pre></td></tr></table></figure>

<p>The compilation process may take a while, so please be patient and get the tools&#x2F;bin folder after compiling</p>
<p><img src="/../images/image-20220826021655278.png" srcset="/img/loading.gif" lazyload alt="image-20220826021655278"></p>
<h3 id="copy-synthesis"><a href="#copy-synthesis" class="headerlink" title="copy synthesis"></a>copy synthesis</h3><p>Synthesize syn.wav with world vocoder based on input test.wav</p>
<p>Extract <code>f0</code>, <code>sp</code>, <code>ap</code> from <code>test.wav</code> using world, then synthesize <code>copy_synthesize/16k_wav_syn/000001.resyn.wav</code> based on the extracted features</p>
<h3 id="Sample-rate-16k"><a href="#Sample-rate-16k" class="headerlink" title="Sample rate 16k"></a>Sample rate 16k</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash copy_synthesize/copy_synthesis_world_16k.sh<br></code></pre></td></tr></table></figure>

<h3 id="sample-rate-48k"><a href="#sample-rate-48k" class="headerlink" title="sample rate 48k"></a>sample rate 48k</h3><p>Modify on the basis of <code>copy_synthesis_world_16k.sh</code>, you can modify parameters as input and output paths <code>(wav_dir\out_dir)</code>, <code>sample rate fs</code>, <code>mcsize</code>, etc</p>
<h3 id="optional-melspectrogram-copy-synthesis"><a href="#optional-melspectrogram-copy-synthesis" class="headerlink" title="(optional) melspectrogram copy synthesis"></a>(optional) melspectrogram copy synthesis</h3><p>Synthesize <code>syn_mel.wav</code> from input <code>test.wav</code> via <code>griffinlim</code></p>
<p>Extract the melspectrogram of test.wav using world, then synthesize copy_synthesize&#x2F;syn_mel.wav based on the extracted melspectrogram features</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python copy_synthesize/copy_synthesis_mel.py copy_synthesize/16k_wav/000001.wav<br></code></pre></td></tr></table></figure>





<h3 id="NN-based-Vocoder"><a href="#NN-based-Vocoder" class="headerlink" title="NN based Vocoder"></a>NN based Vocoder</h3><h1 id="Tacotron-TTS"><a href="#Tacotron-TTS" class="headerlink" title="Tacotron TTS"></a>Tacotron TTS</h1><p><img src="/../images/image-20220825235943493.png" srcset="/img/loading.gif" lazyload alt="image-20220825235943493"></p>
<h2 id="Tacotron-Hands-on"><a href="#Tacotron-Hands-on" class="headerlink" title="Tacotron Hands-on"></a>Tacotron Hands-on</h2><h3 id="Clone-the-code"><a href="#Clone-the-code" class="headerlink" title="Clone the code"></a>Clone the code</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">https://github.com/xiao11lam/TTS.git<br></code></pre></td></tr></table></figure>

<p>Or we can clone here:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs git">https://github.com/nwpuaslp/TTS_Course.git<br></code></pre></td></tr></table></figure>











<h1 id="ESPNET-Text-to-Speech"><a href="#ESPNET-Text-to-Speech" class="headerlink" title="ESPNET Text to Speech"></a>ESPNET Text to Speech</h1><h2 id="Install-ESPNET"><a href="#Install-ESPNET" class="headerlink" title="Install ESPNET"></a>Install ESPNET</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">conda create --name espnet<br></code></pre></td></tr></table></figure>


              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Speech Synthesis Toolkits Introduction</div>
      <div>http://xiaos.site/2022/08/25/Speech-Synthesis-Toolkits-Introduction/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Xiao Zhang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>August 25, 2022</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>Licensed under</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/08/29/CMake-for-Audio-and-Speech-Processing/" title="CMake for Audio and Speech Processing">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CMake for Audio and Speech Processing</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/08/25/Speech-Signal-Processing-Toolkit-SPTK-Tutorials/" title="Speech Signal Processing Toolkit (SPTK) Tutorials">
                        <span class="hidden-mobile">Speech Signal Processing Toolkit (SPTK) Tutorials</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    

  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>© 2022 By Xiao Zhang</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        Views: 
        <span id="busuanzi_value_site_pv"></span>
        
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        Visitors: 
        <span id="busuanzi_value_site_uv"></span>
        
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  

  

  

  

  

  

  




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        MathJax = {
          tex    : {
            inlineMath: { '[+]': [['$', '$']] }
          },
          loader : {
            load: ['ui/lazy']
          },
          options: {
            renderActions: {
              findScript    : [10, doc => {
                document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                  const display = !!node.type.match(/; *mode=display/);
                  const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                  const text = document.createTextNode('');
                  node.parentNode.replaceChild(text, node);
                  math.start = { node: text, delim: '', n: 0 };
                  math.end = { node: text, delim: '', n: 0 };
                  doc.math.push(math);
                });
              }, '', false],
              insertedScript: [200, () => {
                document.querySelectorAll('mjx-container').forEach(node => {
                  let target = node.parentNode;
                  if (target.nodeName.toLowerCase() === 'li') {
                    target.parentNode.classList.add('has-jax');
                  }
                });
              }, '', false]
            }
          }
        };
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
  <!-- Head tag -->
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Title -->
  
  <title>Wenet ASR Toolkit Tutorials From Training to Deployment - Xiao Zhang (Software Design Office)</title>

  <!--Favicon-->
  <link rel="icon" href="favicon/favicon.ico">

  <!--Description-->
  
      <meta name="description" content="1. Use Wenet to train the ASR E2E Model1.1 Pre-trained ModelWe can Download the pre-compiled runtime wenet mode from the wenet github, since they alre">
  

  <!--Author-->
  
      <meta name="author" content="Xiao Zhang">
  

  <!-- Pure CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
  <link href="https://fonts.googleapis.com/css?family=Crimson+Text|Open+Sans:300,800" rel="stylesheet">

  <!-- Custom CSS -->
  
<link rel="stylesheet" href="/css/styles.css">


  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <!-- Google Analytics -->
  

<meta name="generator" content="Hexo 6.2.0"></head>


  <body>
  	<div class="container-fluid navbar-container m-sm-5">
      <!-- Header -->
      <nav class="navbar navbar-toggleable-sm navbar-light px-1 py-3 my-3 mb-sm-5">
  <a class="navbar-brand ml-2" href="/">Xiao Zhang (Software Design Office)</a>
  <button class="navbar-toggler navbar-toggler-right py-2" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse text-center" id="navbarCollapse">
    <ul class="navbar-nav ml-auto my-auto">
      
        <li class="nav-item">
          <a class="nav-link" href="/about">About</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" target="_blank" rel="noopener" href="https://www.linkedin.com/in/xiao-zhang-28785b19b/">LinkedIn</a>
        </li>
      
    </ul>
    <hr class="hidden-md-up" />
  </div>
</nav>


  		<div class="row">
  			<!-- <div class="col-12 mb-4"> -->
<!--   <img class="img-fluid project-img" src="/" alt="Wenet ASR Toolkit Tutorials From Training to Deployment"> -->
<!-- </div> -->
<!-- <div class="col-lg-4 col-12 pt-3 px-4 pr-lg-5"> -->
<!--   <h1>Wenet ASR Toolkit Tutorials From Training to Deployment</h1> -->
<!-- </div> -->
<div class="col-lg-8 col-12 pt-lg-3 mb-4 pl-lg-5 px-lg-0 px-4 portfolio-content">
  <p><img src="/../images/image-20220730230852670.png" alt="image-20220730230852670"></p>
<h1 id="1-Use-Wenet-to-train-the-ASR-E2E-Model"><a href="#1-Use-Wenet-to-train-the-ASR-E2E-Model" class="headerlink" title="1. Use Wenet to train the ASR E2E Model"></a>1. Use Wenet to train the ASR E2E Model</h1><h2 id="1-1-Pre-trained-Model"><a href="#1-1-Pre-trained-Model" class="headerlink" title="1.1 Pre-trained Model"></a>1.1 Pre-trained Model</h2><p>We can Download the pre-compiled runtime wenet mode from the wenet github, since they already shared the pre-trained runtime model <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/wenet/actions/runs/2175816175">https://github.com/wenet-e2e/wenet/actions/runs/2175816175</a> their, so we can just use it instead of compile for ourselves.</p>
<p>Wenet supports the <code>.zip</code> model to decode the ASR outputs. If you do not need to train your model yourself, then you can try this step. </p>
<p><img src="/../images/image-20220730003510616-16591147216542.png" alt="image-20220730003510616"></p>
<h2 id="1-2-Self-trained-Model"><a href="#1-2-Self-trained-Model" class="headerlink" title="1.2 Self-trained Model"></a>1.2 Self-trained Model</h2><p>If you want to train your model, you can try to run an example project just like kaldi <code>wsj</code> examples. In Wenet, since it was created mostly for Chinese ASR tasks, so we will use a Chinese corpus to be the example. But Wenet still supports other languages like English, so if you still want to do it, it should be without any concern. </p>
<p>In all, this is just an example we can refer, the goal is to know how wenet train the ASR we want.</p>
<p>Here is the offical website for the trainning purposes, <a target="_blank" rel="noopener" href="https://wenet.org.cn/wenet/tutorial_aishell.html">https://wenet.org.cn/wenet/tutorial_aishell.html</a></p>
<p>Also, with the LibriSpeech one: <a target="_blank" rel="noopener" href="https://wenet.org.cn/wenet/tutorial_librispeech.html">https://wenet.org.cn/wenet/tutorial_librispeech.html</a></p>
<h1 id="2-Optionally-Integrate-with-the-Language-Model-LM"><a href="#2-Optionally-Integrate-with-the-Language-Model-LM" class="headerlink" title="2. Optionally Integrate with the Language Model (LM)"></a>2. Optionally Integrate with the Language Model (LM)</h1><p>As we know, the wenet will output a <code>TLG.fst</code> decoding model, and then we can just use this model to integrate into the n-gram language model.</p>
<p>Since before we had the dictionary in tokens, but here we are dealing with the word level.</p>
<p>If we use the LM, in most cases the decoding time will be much longer, but also there will be a higher accurancy, but this is not compulsory!</p>
<h2 id="2-1-N-gram-LM-in-Wenet"><a href="#2-1-N-gram-LM-in-Wenet" class="headerlink" title="2.1 N-gram LM in Wenet"></a>2.1 N-gram LM in Wenet</h2><p>Wenet use the N-gram LM to help us to improve our CTC DNN model, since if we can add one more LM so we can have a more promissing WER during the decoding.</p>
<p>Compared with the DNN LM, the n-gram model is very easily to implement and light-weighted, which is much faster than the DNN decoding. And at the same time, the traditional n-gram model is not limited to the corpus size, we can even use a very small corpus to build this. Also compared with DNN, we can make our model more controllable, since DNN is very randomize. </p>
<p>In Wenet, we will import the WFST to do the decoding for the LM. </p>
<h3 id="TLG-fst"><a href="#TLG-fst" class="headerlink" title="TLG.fst"></a>TLG.fst</h3><p>Just like in Kaldi, we have the <code>HCLG.fst</code>, also in Wenet, we have the same strategy. </p>
<p><strong>T.fst</strong></p>
<p>For <code>T</code>,  it means the <code>token</code>, in wenet, it is used for CTC decoding. It is used for: </p>
<ol>
<li>remove and manage the blanks, from <blank> -&gt; <eps> </li>
<li>decode the multiple tokens output into one sentence, how we manage it? (this is very typically used in Mandarin Chinese, which can be a kind of characteristics of that language.)</li>
</ol>
<p><strong>L.fst</strong></p>
<p>L is lexicon, since the logic here is to make the input characters into the words, in english can be phones to words.</p>
<p><strong>G.fst</strong></p>
<p>G is grammar, it guides us from the words level into the sentence level. For words, it can be n-gram model. </p>
<p><strong>LG.fst</strong>  </p>
<p>LG.fst &#x3D; compose(L.fst, G.fst), we can compose those two fst models into one. </p>
<p><strong>TLG.fst</strong></p>
<p>Finally, we can just compose the LG.fst with the T.fst into a big decoding map. </p>
<h2 id="2-2-Language-Model-Deployment"><a href="#2-2-Language-Model-Deployment" class="headerlink" title="2.2 Language Model Deployment"></a>2.2 Language Model Deployment</h2><h3 id="Install-SRILM"><a href="#Install-SRILM" class="headerlink" title="Install SRILM"></a>Install SRILM</h3><p>If you want to review more ideas about the SRILM, you can also refer this page: <a target="_blank" rel="noopener" href="https://www.xiaos.site/2022/07/11/Kaldi-for-Dummies/#2-4-1-write-get-lm-sh-we-need-to-write-aw-shell-script-to-run-and-get-th-lm-language-model">https://www.xiaos.site/2022/07/11/Kaldi-for-Dummies/#2-4-1-write-get-lm-sh-we-need-to-write-aw-shell-script-to-run-and-get-th-lm-language-model</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> wenet/tools</span><br><span class="line"><span class="comment"># download, install the srilm, compile it.</span></span><br><span class="line">bash install_srilm.sh</span><br><span class="line"><span class="built_in">source</span> env.sh</span><br><span class="line"><span class="comment"># check the dependencies whether in our dev env</span></span><br><span class="line"><span class="built_in">which</span> ngram-count</span><br><span class="line"></span><br><span class="line"><span class="comment"># we just train a 2-gram LM, the input text file is train.txt and the output is the lm.arpa</span></span><br><span class="line">ngram-count -order 2 -text train.txt -lm lm.arpa</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># the train.txt is the original lexicon file, which includes some words like</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># train.txt</span><br><span class="line">I Love You</span><br><span class="line">Wenet</span><br><span class="line">ASR</span><br><span class="line">From Training to Deployment</span><br></pre></td></tr></table></figure>

<p>In that case, it will generate a <code>lm.arpa</code> file, and we can <code>vim lm.arpa</code></p>
<h3 id="Get-the-fst-with-LM-language-model"><a href="#Get-the-fst-with-LM-language-model" class="headerlink" title="Get the .fst with LM ( language model)"></a>Get the .fst with LM ( language model)</h3><p><img src="/../images/image-20220804141633132.png" alt="image-20220804141633132"></p>
<p>Her we need to get into the <code>cd /wenet/examples/aishell/s0</code> folder and <code>vim</code> the <code>run.sh</code> file.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 7.1 Prepare dict</span></span><br><span class="line">unit_file=<span class="variable">$dict</span></span><br><span class="line"><span class="built_in">mkdir</span> -p data/local/dict</span><br><span class="line"><span class="built_in">cp</span> <span class="variable">$unit_file</span> data/local/dict/units.txt</span><br><span class="line">tools/fst/prepare_dict.py <span class="variable">$unit_file</span> <span class="variable">$&#123;data&#125;</span>/resource_aishell/lexicon.txt \</span><br><span class="line">    data/local/dict/lexicon.txt</span><br><span class="line"><span class="comment"># 7.2 Train lm</span></span><br><span class="line">lm=data/local/lm</span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$lm</span></span><br><span class="line">tools/filter_scp.pl data/train/text \</span><br><span class="line">     <span class="variable">$data</span>/data_aishell/transcript/aishell_transcript_v0.8.txt &gt; <span class="variable">$lm</span>/text</span><br><span class="line"><span class="built_in">local</span>/aishell_train_lms.sh</span><br><span class="line"><span class="comment"># 7.3 Build decoding TLG</span></span><br><span class="line">tools/fst/compile_lexicon_token_fst.sh \</span><br><span class="line">    data/local/dict data/local/tmp data/local/lang</span><br><span class="line">tools/fst/make_tlg.sh data/local/lm data/local/lang data/lang_test || <span class="built_in">exit</span> 1;</span><br><span class="line"><span class="comment"># 7.4 Decoding with runtime</span></span><br><span class="line">./tools/decode.sh --nj 16 \</span><br><span class="line">    --beam 15.0 --lattice_beam 7.5 --max_active 7000 \</span><br><span class="line">    --blank_skip_thresh 0.98 --ctc_weight 0.5 --rescoring_weight 1.0 \</span><br><span class="line">    --fst_path data/lang_test/TLG.fst \</span><br><span class="line">    --dict_path data/lang_test/words.txt \</span><br><span class="line">    data/test/wav.scp data/test/text <span class="variable">$dir</span>/final.zip \</span><br><span class="line">    data/lang_test/units.txt <span class="variable">$dir</span>/lm_with_runtime</span><br></pre></td></tr></table></figure>

<p>In wenet, this is the 7th step in the  <code>run.sh</code>,  if runs well, we would get :</p>
<p><code>composing decoding graph TLG.fst succeded</code></p>
<p>We will compile a <code>.fst</code> file into the <code>data/lang_test/</code> folder.</p>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h1 id="3-Runtime-Compiling"><a href="#3-Runtime-Compiling" class="headerlink" title="3. Runtime Compiling"></a>3. Runtime Compiling</h1><h2 id="3-1-Get-the-pre-compiled-runtime-from-Wenet"><a href="#3-1-Get-the-pre-compiled-runtime-from-Wenet" class="headerlink" title="3.1 Get the pre-compiled runtime from Wenet"></a>3.1 Get the pre-compiled runtime from Wenet</h2><p>Wenet already gave us a pre-compiled runtime software for us to decode, so we do not need to compile the runtime ourselves, we can firstly use the wenet official released version to help us to do the decoding in the next stage, we can download here:</p>
<p><img src="/../images/image-20220804222301456.png" alt="image-20220804222301456"></p>
<p>After we downloaded, we can see those documents there:</p>
<p><img src="/../images/image-20220804222653725.png" alt="image-20220804222653725"></p>
<p>We will use mostly those three <code>.exe</code>  programs with red marks for us to do the decoding. </p>
<h2 id="3-2-Compile-at-your-local-machine"><a href="#3-2-Compile-at-your-local-machine" class="headerlink" title="3.2 Compile at your local machine"></a>3.2 Compile at your local machine</h2><p>If you want to compile your own runtime files on your local machine <strong>instead of downloading from the pre-compiled version from the Wenet official release</strong>, please check here!</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> runtime/server/x86</span><br><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build &amp;&amp; cmake .. &amp;&amp; cmake --build .</span><br></pre></td></tr></table></figure>

<p>There is a notice here, we must ensure the <strong>cmake is upper than the 3.14 version, and the gcc should be higher than 5.4.</strong> </p>
<h1 id="4-Decode-the-Model"><a href="#4-Decode-the-Model" class="headerlink" title="4. Decode the Model"></a>4. Decode the Model</h1><p>Here is an example, we will use the pre-trained wenet model to do a sample decoding. After we downloaded that pre-trained model, we can just get into that downloaded directory.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> Downloads/release-wenet-binary</span><br></pre></td></tr></table></figure>



<h2 id="4-1-Runtime-Local-Decoding"><a href="#4-1-Runtime-Local-Decoding" class="headerlink" title="4.1 Runtime Local Decoding"></a>4.1 Runtime Local Decoding</h2><p>Here we can do a basic decoding test from our downloaded pre-trained runtime model. The decoding tool is just like <code>decoder_main.exe</code>, but we have to pass in some params there. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--chunk_size    the wave chunck streamming param</span><br><span class="line">--wav_path		the wave file we want to do the decoding</span><br><span class="line">--model_path	the neural model from the trained model</span><br><span class="line">--dict_path     the dictionary file from the trained model.</span><br></pre></td></tr></table></figure>



<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./decoder_main.exe     --chunk_size -1     --wav_path ../test.wav     --model_path ../20210601_u2++_conformer_libtorch/20210601_u2++_conformer_libtorch/final.zip     --dict_path ../20210601_u2++_conformer_libtorch/20210601_u2++_conformer_libtorch/units.txt</span><br></pre></td></tr></table></figure>



<p>Here is the demo code that wenet gave:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/wenet/runtime/server/x86</span><br><span class="line"><span class="built_in">export</span> GLOG_logtostderr=1</span><br><span class="line"><span class="built_in">export</span> GLOG_v=2</span><br><span class="line">wav_path=../test.wav</span><br><span class="line">model_dir=../20210601_u2++_conformer_libtorch/20210601_u2++_conformer_libtorch</span><br><span class="line">./build/bin/decoder_main \</span><br><span class="line">    --chunk_size 16 \</span><br><span class="line">    --wav_path <span class="variable">$wav_path</span> \</span><br><span class="line">    --model_path <span class="variable">$model_dir</span>/final.zip \</span><br><span class="line">    --unit_path <span class="variable">$model_dir</span>/units.txt 2&gt;&amp;1 | <span class="built_in">tee</span> log.txt \</span><br><span class="line">    <span class="comment"># here are the path need to specify if we need the LM, it without these two lines of code, it will decode without the LM</span></span><br><span class="line">    --fst_path <span class="variable">$model_dir</span>/TLG.fst \</span><br><span class="line">    --dict_path <span class="variable">$model_dir</span>/words.txt</span><br></pre></td></tr></table></figure>

<h2 id="4-2-Online-Decoding-with-Host"><a href="#4-2-Online-Decoding-with-Host" class="headerlink" title="4.2 Online Decoding with Host"></a>4.2 Online Decoding with Host</h2><h3 id="4-2-1-Runtime-Host-Decoding-with-a-CMD-interface"><a href="#4-2-1-Runtime-Host-Decoding-with-a-CMD-interface" class="headerlink" title="4.2.1 Runtime Host Decoding with a CMD interface"></a>4.2.1 Runtime Host Decoding with a CMD interface</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./websocket_client_main.exe   --hostname 127.0.0.1   --port 10086    --wav_path ../test.wav</span><br></pre></td></tr></table></figure>

<h3 id="4-2-2-Runtime-Host-Decoding-with-a-Web-interface"><a href="#4-2-2-Runtime-Host-Decoding-with-a-Web-interface" class="headerlink" title="4.2.2 Runtime Host Decoding with a Web interface"></a>4.2.2 Runtime Host Decoding with a Web interface</h3><p>Wenet just gave us a great demo for us to do the demo illustration, and it used the <code>Flask</code>.  </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># here are the code to run the runtime server, right now it is hosted, and we can get the request from that url: ws://127.0.0.1:10086</span></span><br><span class="line">./websocket_server_main.exe --port 10086    --chunk_size -1    --model_path ../20210601_u2++_conformer_libtorch/20210601_u2++_conformer_libtorch/final.zip     --dict_path ../20210601_u2++_conformer_libtorch/20210601_u2++_conformer_libtorch/units.txt</span><br></pre></td></tr></table></figure>

<p>For the server side, it will open a server from the local machine at the 10086, we can then get into the <code>index.html</code> from the flask demo to get the response there. </p>
<p><img src="/../images/image-20220730004721190.png" alt="image-20220730004721190"></p>
<p>We need to open the <code>index.html</code> after we already made sure that our <code>ws://127.0.0.1:10086</code> host is launching. </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> wenet/runtime/server/x86/web/templates</span><br><span class="line"><span class="comment"># we can just open that index.html to open a flask web demo app to retreive the data there.</span></span><br></pre></td></tr></table></figure>

<p>The blue button here is a Chinese means “start to do the speech recognition”, and after we click the allow to recording, so we can just do the asr tasks here with the streaming and unstreamming modes only if we changed the <code>chunk_size</code> from our decoding cmds.</p>
<p><img src="/../images/image-20220730004448344.png" alt="image-20220730004448344"></p>
<h2 id="4-3-Use-Python"><a href="#4-3-Use-Python" class="headerlink" title="4.3 Use Python"></a>4.3 Use Python</h2><p>We can refer this website,  it supports the streaming and non-streaming methods: <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/wenet/tree/main/runtime/binding/python">https://github.com/wenet-e2e/wenet/tree/main/runtime/binding/python</a></p>
<p>Here we just choose a <strong>Non-streaming</strong> Usage. </p>
<p>we just firstly install the python:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install wenet</span><br></pre></td></tr></table></figure>

<p>We need to write a <code>demo.py</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># demo.py</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> wenet</span><br><span class="line"></span><br><span class="line">wav_file = sys.argv[<span class="number">1</span>] <span class="comment"># we specify the file we want to decode </span></span><br><span class="line">decoder = wenet.Decoder(model_dir, lang=<span class="string">&#x27;chs&#x27;</span>)</span><br><span class="line"><span class="comment"># in the model directory we need to add 4 files into it: 1) the runtime model like final.zip 2) units.txt from the datasets, 3) words.txt, from the LM we trained (optional), 4) TLG.fst, from previous operations (optional)</span></span><br><span class="line">ans = decoder.decode_wav(wav_file)</span><br><span class="line"><span class="built_in">print</span>(ans)</span><br><span class="line"><span class="comment"># call decoder.reset() if you want to do the next decoding</span></span><br></pre></td></tr></table></figure>





<h1 id="5-Hot-Word-Enhancement"><a href="#5-Hot-Word-Enhancement" class="headerlink" title="5. Hot Word Enhancement"></a>5. Hot Word Enhancement</h1><p>Hot word is contextual (context) biasing, which means we can argumented the specific word we want and make it easily be recognized. This is often occurred within the speech technology. </p>
<p>The common senarios can be recognize: locations, contacter’s name, date or telephone numbers… This senario is always used into the real commercial cases.</p>
<p>In all, the how word enhancement is to give some particular words some kind of more weighting score. In this way, it will make the word we want (hot word) more easily to be decoded.</p>
<h1 id="6-Long-time-audio-ASR"><a href="#6-Long-time-audio-ASR" class="headerlink" title="6. Long-time audio ASR"></a>6. Long-time audio ASR</h1><p><img src="/../images/image-20220819211753969.png" alt="image-20220819211753969"></p>
<p>The basic philosophy here for long-time audio asr tasks are very direct, we just divide the big problems into small problems. We have to detect the endpoint for each sentences, just make it in trucks. And after the end-point,  we can re-recognize the sentences. </p>
<h2 id="6-1-EndPoint-Detection"><a href="#6-1-EndPoint-Detection" class="headerlink" title="6.1 EndPoint Detection"></a>6.1 EndPoint Detection</h2><p>What is endpoint, endpoint is somewhere that the user will stop at some point, or just the end of the sentences. We can just set the silence threshold of the sentences. </p>
<p>There are three senarios(rules) in wenet:</p>
<ol>
<li>Before the speech, the silence in there. Default value is 5000ms (5s).</li>
<li>Within the speech, the silence there. Default value is 1000ms (1s).</li>
<li>If the user’s speech trunks are too large, we have to cut it forcelly. Default value is 20000ms (20s).</li>
</ol>
<p>We can set the params here in wenet.</p>
<p>Here are the core codes here from wenet for Endpoint detection task:<a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/wenet/blob/main/runtime/core/decoder/ctc_endpoint.h">https://github.com/wenet-e2e/wenet/blob/main/runtime/core/decoder/ctc_endpoint.h</a></p>
<p><img src="/../images/image-20220819212902233.png" alt="image-20220819212902233"></p>
<p><img src="/../images/image-20220819213119821.png" alt="image-20220819213119821"></p>
<p>We can define the values we want, the units will be 5000 ms. </p>
<p>The real code for <strong>continous blank detection</strong> is in here: <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/wenet/blob/main/runtime/core/decoder/ctc_endpoint.cc">https://github.com/wenet-e2e/wenet/blob/main/runtime/core/decoder/ctc_endpoint.cc</a></p>
<p>These are the codes that how it really works:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">CtcEndpoint::IsEndpoint</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::vector&lt;std::vector&lt;<span class="type">float</span>&gt;&gt;&amp; ctc_log_probs,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="comment">// it will get the ctc log probabilities, it will get the ctc prob and compare with the blank prob.</span></span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">bool</span> decoded_something)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> t = <span class="number">0</span>; t &lt; ctc_log_probs.<span class="built_in">size</span>(); ++t) &#123;</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>&amp; <span class="type">logp_t</span> = ctc_log_probs[t];</span><br><span class="line">    <span class="type">float</span> blank_prob = <span class="built_in">expf</span>(<span class="type">logp_t</span>[config_.blank]);</span><br><span class="line">    num_frames_decoded_++;</span><br><span class="line">    <span class="comment">// in here if the blank prob bigger than the preset blank value we set, it will detect as silence signal.</span></span><br><span class="line">    <span class="keyword">if</span> (blank_prob &gt; config_.blank_threshold) &#123;</span><br><span class="line">      num_frames_trailing_blank_++;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// if not will be cleared into 0, some it follows a culmuative way.</span></span><br><span class="line">      num_frames_trailing_blank_ = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// we will check the length of the speech.</span></span><br><span class="line">  <span class="built_in">CHECK_GE</span>(num_frames_decoded_, num_frames_trailing_blank_);</span><br><span class="line">  <span class="built_in">CHECK_GT</span>(frame_shift_in_ms_, <span class="number">0</span>);</span><br><span class="line">  <span class="type">int</span> utterance_length = num_frames_decoded_ * frame_shift_in_ms_;</span><br><span class="line">  <span class="type">int</span> trailing_silence = num_frames_trailing_blank_ * frame_shift_in_ms_;</span><br><span class="line">  <span class="comment">// if there is any case that be activated, the endpoint exception case will be triggered.</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">RuleActivated</span>(config_.rule1, <span class="string">&quot;rule1&quot;</span>, decoded_something, trailing_silence,</span><br><span class="line">                    utterance_length))</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">RuleActivated</span>(config_.rule2, <span class="string">&quot;rule2&quot;</span>, decoded_something, trailing_silence,</span><br><span class="line">                    utterance_length))</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">RuleActivated</span>(config_.rule3, <span class="string">&quot;rule3&quot;</span>, decoded_something, trailing_silence,</span><br><span class="line">                    utterance_length))</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="6-1-1-CTC-Endpoint-Process"><a href="#6-1-1-CTC-Endpoint-Process" class="headerlink" title="6.1.1 CTC Endpoint Process"></a>6.1.1 CTC Endpoint Process</h3><h4 id="Endpoint-Detection"><a href="#Endpoint-Detection" class="headerlink" title="Endpoint Detection"></a>Endpoint Detection</h4><p>We can check the code here: <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/wenet/blob/main/runtime/core/decoder/asr_decoder.cc#L117">https://github.com/wenet-e2e/wenet/blob/main/runtime/core/decoder/asr_decoder.cc#L117</a></p>
<p><img src="/../images/image-20220821035221487.png" alt="image-20220821035221487"></p>
<p>Here the program can detect whether there is a endpoint there.</p>
<h4 id="Rescoring"><a href="#Rescoring" class="headerlink" title="Rescoring"></a>Rescoring</h4><p>For Wenet, we have the CTC decoding results to do the rescoring tatics. It will the Nbest scoring, also it will append the recognition results into the original results. <img src="/../images/image-20220821035807719.png" alt="image-20220821035807719"></p>
<p>We can detect from <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/wenet/blob/main/runtime/core/bin/decoder_main.cc#L75">https://github.com/wenet-e2e/wenet/blob/main/runtime/core/bin/decoder_main.cc#L75</a></p>
<h4 id="State-Reset"><a href="#State-Reset" class="headerlink" title="State Reset"></a>State Reset</h4><p><a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/wenet/blob/main/runtime/core/decoder/asr_decoder.cc#L67">https://github.com/wenet-e2e/wenet/blob/main/runtime/core/decoder/asr_decoder.cc#L67</a></p>
<p><img src="/../images/image-20220821040446495.png" alt="image-20220821040446495"></p>
<p>It will do the status reset for the final step. It will clear the status, and ready for the next-stage recognition. It will add a comma “,” for each endpoint.</p>
<h3 id="6-1-2-Webrtcvad"><a href="#6-1-2-Webrtcvad" class="headerlink" title="6.1.2 Webrtcvad"></a>6.1.2 Webrtcvad</h3><p>Wenet also supports like external vad modules like from <code>webrtcvad</code>. It is a very great open-resource job here, the difference here is that it used more tactics to do the audio vad works instead of just pure stop-point detections. </p>
<p><a target="_blank" rel="noopener" href="https://github.com/wiseman/py-webrtcvad">https://github.com/wiseman/py-webrtcvad</a></p>
<p><img src="/../images/image-20220821055530432.png" alt="image-20220821055530432"></p>
<p>The way it works are very direct, it will detect whether there is the silent audio.</p>
<p><img src="/../images/image-20220821055724162.png" alt="From: https://github.com/wiseman/py-webrtcvad/blob/master/example.py#L141"></p>
<h2 id="6-2-Python-Deployment"><a href="#6-2-Python-Deployment" class="headerlink" title="6.2 Python Deployment"></a>6.2 Python Deployment</h2><p>The easiest way is to just call the <code>continous_decoding</code> param and set it as <code>True</code>. If we set this as <code>False</code>, there can be no stops of our recognition texts, and at the same time, there will be a faster speed to decode since there is no endstop detection.</p>
<p><img src="/../images/image-20220821052226122.png" alt="image-20220821052226122"></p>
<p>We will use a streaming way that we will input the audio chunks in every 0.5 seconds. We decode the audio file in each 0.5 seconds each time, it will like the streaming way. The delay in the streaming ASR jobs will always like between <code>500ms</code> to <code>600ms</code>, so it will be limited on there speech recognition accuracy.</p>
<h2 id="6-3-Use-C"><a href="#6-3-Use-C" class="headerlink" title="6.3 Use C++"></a>6.3 Use C++</h2><p>We can easily use the C++ to do the long-time audio ASR by just selecting the <code>continous_decoding</code> into <code>True</code>:</p>
<p><img src="/../images/image-20220821053452850.png" alt="image-20220821053452850"></p>

</div>


<style>
  .portfolio-content img {
    width: 100%;      /* Make the image width equal to the container width */
    height: auto;     /* Maintain the aspect ratio */
    max-width: 100%;  /* Ensures the image does not exceed the container width */
  }
</style>





      </div>
      
  	</div>

    <!-- After footer scripts -->
    <script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>

  </body>
</html>
